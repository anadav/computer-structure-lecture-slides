\documentclass[aspectratio=169]{beamer}
\usetheme{Madrid}
\usecolortheme{whale}

\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{tikz}
\usepackage[siunitx]{circuitikz}
\usetikzlibrary{positioning, matrix, fit, backgrounds, shapes.callouts, tikzmark, calc, shapes.geometric, shapes.symbols, overlay-beamer-styles}
\usepackage{pgfplots}
\pgfplotsset{compat=1.18}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{arydshln}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{minted}

\lstset{
    basicstyle=\ttfamily\tiny,
    keywordstyle=\color{blue}\bfseries,
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    showstringspaces=false,
    breaklines=true,
    frame=single,
    backgroundcolor=\color{gray!10},
    language=C++,
    morekeywords={__global__, __device__, __shared__, half, float, wmma, fragment}
}

\title{GPU and Other Accelerators}
\subtitle{Performance Engineering}
\author{Technion}
\date{}

\AtBeginSection[]{
    \begin{frame}{Outline}
        \tableofcontents[currentsection]
    \end{frame}
}

\begin{document}

\begin{frame}
    \titlepage
\end{frame}

\begin{frame}{Outline}
    \tableofcontents
\end{frame}

%===============================================================================
\section{Introduction to GPGPU}
%===============================================================================

\begin{frame}{What is GPGPU?}
    \textbf{General-Purpose computing on Graphics Processing Units}

    \vspace{1em}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Why GPUs?}
            \begin{itemize}
                \item Massive parallelism
                \item High memory bandwidth
                \item Energy efficiency
                \item Specialized units (Tensor cores)
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Suitable Workloads:}
            \begin{itemize}
                \item Data-parallel algorithms (linear algebra, FFTs)
                \item Embarrassingly parallel tasks (Monte Carlo)
                \item Image/video processing
                \item AI/ML inference and training
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{1em}
    \textbf{Mature Ecosystem:}
    \begin{itemize}
        \item APIs: CUDA, OpenCL
        \item Libraries: cuBLAS, cuDNN, Thrust
        \item Frameworks: TensorFlow, PyTorch
    \end{itemize}
\end{frame}

\begin{frame}{When to Use GPU?}
    \textbf{Arithmetic Intensity:}
    \[
    I = \frac{\text{FLOPs}}{\text{bytes accessed}}
    \]

    \begin{itemize}
        \item $I \gtrsim 5$--$10$ FLOP/byte $\rightarrow$ Almost certainly \alert{compute-bound}
        \item Also known as ``Operational Intensity''
    \end{itemize}

    \vspace{1em}
    \textbf{Can still benefit with low AI:}
    \begin{itemize}
        \item Massive parallelism hides latency
        \item High-throughput HBM memory
        \item Overlap of compute \& memory operations
    \end{itemize}
\end{frame}

\begin{frame}{Roofline Model}
    Visual model for estimating performance of compute kernels.

    \begin{columns}
        \begin{column}{0.35\textwidth}
            \textbf{Key Components:}
            \begin{itemize}
                \item Maximum performance ($\pi$)
                \item Arithmetic intensity ($I$)
                \item Memory bandwidth ($\beta$)
            \end{itemize}

            \textbf{Performance bound:}
            \[
            P = \min\left\{ \pi, \; \beta \cdot I \right\}
            \]

            Highlights hardware limits and guides optimization priorities.
        \end{column}
        \begin{column}{0.65\textwidth}
            \begin{center}
            \begin{tikzpicture}[scale=0.8]
            \begin{axis}[
                name=main,
                width=12cm,
                height=7cm,
                xmode=log,
                ymode=log,
                log basis x=2,
                log basis y=2,
                xmin=0.125, xmax=128,
                ymin=0.125, ymax=2.8,
                xlabel={Operational Intensity [FLOPs/byte]},
                ylabel={Performance [GFLOPs]},
                xtick={0.25, 0.5, 1, 2, 4, 8, 16, 32, 64, 128},
                xticklabels={$\frac{1}{4}$, $\frac{1}{2}$, 1, 2, 4, 8, 16, 32, 64, 128},
                ytick={0.25, 0.5, 1, 2},
                yticklabels={$\frac{1}{4}$, $\frac{1}{2}$, 1, 2},
                axis lines=left,
                axis line style={-},
                tick style={black},
                grid=none,
                clip=false,
            ]
            % Dashed horizontal line behind pi
            \addplot[black, dashed, thin, domain=0.125:8, samples=2] {2};
            % Diagonal line (memory-bound): P = beta * I
            \addplot[black, thick, domain=0.5:8, samples=2] {0.25*x};
            % Horizontal line (compute-bound): P = pi = 2
            \addplot[black, thick, domain=8:128, samples=2] {2};
            % Dashed extension of diagonal beyond ridge
            \addplot[black, dashed, thin, domain=8:16, samples=2] {0.25*x};
            % Vertical dashed line O_1 (memory-bound)
            \addplot[red, dashed, thick] coordinates {(4, 0.125) (4, 1)};
            % Vertical dashed line O_2 (compute-bound)
            \addplot[red, dashed, thick] coordinates {(16, 0.125) (16, 2)};
            % Labels
            \node[red, rotate=90, anchor=north west, font=\footnotesize] at (axis cs:4.3,0.13) {$O_1$ (memory-bound)};
            \node[red, rotate=90, anchor=north west, font=\footnotesize] at (axis cs:16.5,0.13) {$O_2$ (compute-bound)};
            \node[anchor=south, font=\large] at (axis cs:45,2.05) {$\pi$};
            \path (axis cs:0.5,0.125) coordinate (A);
            \path (axis cs:8,2) coordinate (B);
            \end{axis}
            % Beta label with calculated angle
            \pgfmathanglebetweenpoints{\pgfpointanchor{A}{center}}{\pgfpointanchor{B}{center}}
            \let\betaangle\pgfmathresult
            \node[rotate=\betaangle, anchor=south, font=\large] at ($(A)!0.35!(B) + (-0.5,0.15)$) {$\beta \cdot I$};
            \end{tikzpicture}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

%===============================================================================
\section{GPU Architecture}
%===============================================================================

\begin{frame}{NVIDIA Blackwell Ultra --- Full Chip}
    \begin{center}
    \begin{tikzpicture}[
        % Styles - all components fully colored, highlights added per stage
        gpu/.style={minimum width=100mm, minimum height=46mm},
        die/.style={draw, thick, fill=olive!25, minimum width=42mm, minimum height=40mm},
        hbm/.style={draw, fill=purple!40, minimum width=18mm, minimum height=4mm, font=\fontsize{4}{5}\selectfont, align=center},
        gpc/.style={draw, fill=olive!50, minimum width=12mm, minimum height=8mm, font=\fontsize{5}{6}\selectfont, align=center},
        l2/.style={draw, fill=olive!70, minimum width=6mm, minimum height=8mm, font=\fontsize{5}{6}\selectfont, align=center},
        engine/.style={draw, fill=yellow!30, minimum width=32mm, minimum height=5mm, font=\fontsize{4}{5}\selectfont, align=center},
        nvdec/.style={draw, fill=gray!30, minimum width=4mm, minimum height=5mm, font=\fontsize{3}{4}\selectfont, align=center},
        nvlinkbar/.style={draw, fill=green!40, text width=6mm, minimum height=32mm, font=\fontsize{4}{5}\selectfont, align=center, inner sep=1pt},
        hbi/.style={draw, fill=yellow!50, minimum width=3mm, minimum height=40mm, font=\fontsize{5}{6}\selectfont, align=center},
        pcie/.style={draw, fill=gray!40, text width=6mm, minimum height=8mm, font=\fontsize{3}{4}\selectfont, align=center, inner sep=1pt},
        callout/.style={rectangle callout, draw=blue!70, fill=blue!10, font=\fontsize{6}{7}\selectfont, align=center, rounded corners=1pt},
        infobox/.style={draw=blue!70, fill=blue!10, rounded corners=3pt, font=\small, align=center, text width=90mm, inner sep=3pt},
        highlight/.style={draw=red, very thick, rounded corners=2pt},
    ]
        % === GPU (outer rectangle) ===
        \node[gpu] (gpu) {};

        % === Two dies inside GPU ===
        \node[die, anchor=east] (dieL) at ([xshift=-4mm]gpu.center) {};
        \node[die, anchor=west] (dieR) at ([xshift=4mm]gpu.center) {};

        % === High Bandwidth Interface between dies (two narrow bars) ===
        \node[hbi, anchor=east] (hbiL) at (gpu.center) {\rotatebox{90}{NV-HBI}};
        \node[hbi, anchor=west] (hbiR) at (gpu.center) {\rotatebox{-90}{NV-HBI}};

        % === LEFT DIE CONTENTS ===

        % Top HBM row: 2 controllers
        \node[hbm, anchor=north] (hbmLT0) at ([xshift=-8mm]dieL.north) {HBM CTRL};
        \node[hbm, anchor=north] (hbmLT1) at ([xshift=12mm]dieL.north) {HBM CTRL};

        % Bottom HBM row: 2 controllers
        \node[hbm, anchor=south] (hbmLB0) at ([xshift=-8mm]dieL.south) {HBM CTRL};
        \node[hbm, anchor=south] (hbmLB1) at ([xshift=12mm]dieL.south) {HBM CTRL};

        % NV DEC (west of NV-HBI)
        \node[nvdec, anchor=east] (nvdecL) at (hbiL.west) {NV\\DEC};

        % === Top row: chain from NV-DEC outward: GPC | L2 | GPC ===
        % Top right GPC (adjacent to NV-DEC)
        \node[gpc, minimum height=13mm, anchor=south east, inner sep=1pt] (gpcL1) at (nvdecL.north east) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};
        % Top L2 (adjacent to top right GPC)
        \node[l2, minimum height=13mm, anchor=south east] (l2L0) at (gpcL1.south west) {L2};
        % Top left GPC (adjacent to L2)
        \node[gpc, minimum height=13mm, anchor=south east, inner sep=1pt] (gpcL0) at (l2L0.south west) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};

        % === Bottom row: chain from NV-DEC outward: GPC | L2 | GPC ===
        % Bottom right GPC (adjacent to NV-DEC)
        \node[gpc, minimum height=13mm, anchor=north east, inner sep=1pt] (gpcL3) at (nvdecL.south east) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};
        % Bottom L2 (adjacent to bottom right GPC)
        \node[l2, minimum height=13mm, anchor=north east] (l2L1) at (gpcL3.north west) {L2};
        % Bottom left GPC (adjacent to L2)
        \node[gpc, minimum height=13mm, anchor=north east, inner sep=1pt] (gpcL2) at (l2L1.north west) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};

        % Engine: width computed from gpcL0.west to nvdecL.west
        \path let \p1=(gpcL0.west), \p2=(nvdecL.west) in
            node[engine, minimum width={\x2-\x1}, anchor=east, inner sep=0pt] (engineL) at (nvdecL.west) {GIGATHREAD ENGINE\\w/ MIG CONTROL};

        % === NVLink bar (adjacent to engine, shifted down by HBM height) ===
        \node[nvlinkbar, anchor=east] (nvlink) at ([yshift=-4mm]engineL.west) {NV\\Link};

        % === PCIe (adjacent to NVLink, above it) ===
        \node[pcie, font=\fontsize{5}{6}\selectfont, anchor=south] (pcie) at (nvlink.north) {PCIe\\Gen 6};


        % === RIGHT DIE CONTENTS ===

        % Top HBM row: 2 controllers
        \node[hbm, anchor=north] (hbmRT0) at ([xshift=-12mm]dieR.north) {HBM CTRL};
        \node[hbm, anchor=north] (hbmRT1) at ([xshift=8mm]dieR.north) {HBM CTRL};

        % Bottom HBM row: 2 controllers
        \node[hbm, anchor=south] (hbmRB0) at ([xshift=-12mm]dieR.south) {HBM CTRL};
        \node[hbm, anchor=south] (hbmRB1) at ([xshift=8mm]dieR.south) {HBM CTRL};

        % NV DEC (east of NV-HBI)
        \node[nvdec, anchor=west] (nvdecR) at (hbiR.east) {NV\\DEC};

        % === Top row: chain from NV-DEC outward: GPC | L2 | GPC ===
        % Top left GPC (adjacent to NV-DEC)
        \node[gpc, minimum height=13mm, anchor=south west, inner sep=1pt] (gpcR0) at (nvdecR.north west) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};
        % Top L2 (adjacent to top left GPC)
        \node[l2, minimum height=13mm, anchor=south west] (l2R0) at (gpcR0.south east) {L2};
        % Top right GPC (adjacent to L2)
        \node[gpc, minimum height=13mm, anchor=south west, inner sep=1pt] (gpcR1) at (l2R0.south east) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};

        % === Bottom row: chain from NV-DEC outward: GPC | L2 | GPC ===
        % Bottom left GPC (adjacent to NV-DEC)
        \node[gpc, minimum height=13mm, anchor=north west, inner sep=1pt] (gpcR2) at (nvdecR.south west) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};
        % Bottom L2 (adjacent to bottom left GPC)
        \node[l2, minimum height=13mm, anchor=north west] (l2R1) at (gpcR2.north east) {L2};
        % Bottom right GPC (adjacent to L2)
        \node[gpc, minimum height=13mm, anchor=north west, inner sep=1pt] (gpcR3) at (l2R1.north east) {GRAPHICS\\PROCESSOR\\CLUSTER\\(GPC)};

        % Engine: width computed from nvdecR.east to gpcR1.east
        \path let \p1=(nvdecR.east), \p2=(gpcR1.east) in
            node[engine, minimum width={\x2-\x1}, anchor=west, inner sep=0pt] (engineR) at (nvdecR.east) {GIGATHREAD ENGINE\\w/ MIG CONTROL};

        % === NVLink-C2C bar (adjacent to engine) ===
        \node[nvlinkbar, fill=cyan!40, anchor=west] (c2c) at (engineR.east) {NVLink\\C2C};

        % === STAGE HIGHLIGHTS (red outlines) ===
        % Stage 1: Two dies (excluding NV-HBI)
        \only<1>{
            \node[highlight, fit=(pcie) (nvlink) (hbmLT0) (hbmLT1) (hbmLB0) (hbmLB1) (gpcL0) (gpcL1) (engineL)] {};
            \node[highlight, fit=(c2c) (hbmRT0) (hbmRT1) (hbmRB0) (hbmRB1) (gpcR0) (gpcR1) (engineR)] {};
        }
        % Stage 2: GPCs + L2
        \only<2>{
            \node[highlight, fit=(gpcL0) (gpcL1) (l2L0)] {};
            \node[highlight, fit=(gpcL2) (gpcL3) (l2L1)] {};
            \node[highlight, fit=(gpcR0) (gpcR1) (l2R0)] {};
            \node[highlight, fit=(gpcR2) (gpcR3) (l2R1)] {};
        }
        % Stage 3: HBM
        \only<3>{
            \node[highlight, fit=(hbmLT0) (hbmLT1)] {};
            \node[highlight, fit=(hbmLB0) (hbmLB1)] {};
            \node[highlight, fit=(hbmRT0) (hbmRT1)] {};
            \node[highlight, fit=(hbmRB0) (hbmRB1)] {};
        }
        % Stage 4: NVLink + C2C + PCIe
        \only<4>{
            \node[highlight, fit=(nvlink) (pcie)] {};
            \node[highlight, fit=(c2c)] {};
        }
        % Stage 5: Engines only
        \only<5>{
            \node[highlight, fit=(engineL)] {};
            \node[highlight, fit=(engineR)] {};
        }

        % === INFO BOX at bottom (changes per stage) ===
        \only<1>{\node[infobox, anchor=north] at ([yshift=-3mm]gpu.south) {\textbf{Dual-Die Architecture:} Two dies connected via NV-HBI with 10 TB/s bandwidth, doubling compute density.};}
        \only<2>{\node[infobox, anchor=north] at ([yshift=-3mm]gpu.south) {\textbf{Massive Parallelism:} 160 SMs total (8 GPCs/die $\times$ 2), 640 Tensor Cores, 15 PetaFLOPS AI compute.};}
        \only<3>{\node[infobox, anchor=north] at ([yshift=-3mm]gpu.south) {\textbf{High Bandwidth Memory:} 288GB HBM3E (12 stacks) at 8 TB/s --- essential for large AI models.};}
        \only<4>{\node[infobox, anchor=north] at ([yshift=-3mm]gpu.south) {\textbf{Interconnects:} NVLink connects GPUs (1.8 TB/s); C2C provides \emph{cache-coherent} CPU-GPU link (shared memory, no copies).};}
        \only<5>{\node[infobox, anchor=north] at ([yshift=-3mm]gpu.south) {\textbf{Hardware Scheduling:} GigaThread Engine distributes blocks across SMs; MIG partitions for multi-tenancy.};}

    \end{tikzpicture}
    \end{center}
\end{frame}

\begin{frame}<0>{NVIDIA Blackwell Ultra --- Full Chip (Original)}
    \begin{center}
        \includegraphics[height=0.85\textheight]{figures/NVIDIA-Blackwell-Ultra-GPU-chip-explained-png.png}
    \end{center}
\end{frame}

\begin{frame}{NVIDIA Blackwell Architecture (2024)}
    \textbf{NVIDIA B200 (Blackwell Architecture):}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Top Level:}
            \begin{itemize}
                \item 208 billion transistors
                \item Dual-die design (2$\times$ GB100)
                \item 10 TB/s chip-to-chip interconnect (NV-HBI)
                \item 148 SMs
                \item 192GB HBM3e @ 8 TB/s
            \end{itemize}

            \vspace{0.5em}
            \textbf{Interconnects:}
            \begin{itemize}
                \item NVLink 5.0: 1.8 TB/s
                \item PCIe Gen5: 128 GB/s
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Trends across GPU generations:}
            \begin{itemize}
                \item Tensor Cores increasingly dominant over CUDA Cores
                \item Lower precision formats (FP8 $\rightarrow$ FP4)
                \item Dedicated memory for Tensor Cores (TMEM)
                \item Multi-die designs to bypass reticle limits
                \item Memory bandwidth scaling (HBM3 $\rightarrow$ HBM3e)
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{0.3em}
    {\scriptsize Source: NVIDIA Blackwell Tuning Guide \& GTC 2024}
\end{frame}

\begin{frame}{NVIDIA Blackwell Ultra --- SM Architecture}
    \begin{columns}[T]
        \begin{column}{0.55\textwidth}
            \vspace{-0.5em}
            \onslide<1->{\textbf{Per SM (Streaming Multiprocessor):}}
            \begin{itemize}
                \item<1-> 128 FP32 \tikz[baseline=-0.5ex]{\node[fill=green!60, draw, rounded corners=1pt, inner sep=1pt, font=\scriptsize] {CUDA Cores};}
                \item<1-> 4 Gen5 \tikz[baseline=-0.5ex]{\node[fill=green!60, draw, rounded corners=1pt, inner sep=1pt, font=\scriptsize] {Tensor Cores};}
                \item<1-> 4 \tikz[baseline=-0.5ex]{\node[fill=orange!50, draw, rounded corners=1pt, inner sep=1pt, font=\scriptsize] {Warp Schedulers};} (64 warps max)
            \end{itemize}

            \vspace{0.5em}
            \onslide<2->{\textbf{On-Chip Memory per SM:}}
            \begin{itemize}
                \item<2-> 256KB \tikz[baseline=-0.5ex]{\node[fill=blue!60, draw, rounded corners=1pt, inner sep=1pt, font=\scriptsize, text=white] {Register File};}
                \item<2-> 228KB \tikz[baseline=-0.5ex]{\node[fill=blue!30, draw, rounded corners=1pt, inner sep=1pt, font=\scriptsize] {Shared Memory / L1};}
                \item<2-> 256KB \tikz[baseline=-0.5ex]{\node[fill=cyan!50, draw, rounded corners=1pt, inner sep=1pt, font=\scriptsize] {Tensor Memory};}
            \end{itemize}

            \vspace{0.5em}
            \onslide<3->{\textbf{Memory Hierarchy:}}
            \begin{itemize}
                \item<3-> \textbf{Per Thread:} Registers (max 255)
                \item<3-> \textbf{Per Block:} Shared Memory
                \item<3-> \textbf{All SMs:} L2 Cache, Global Memory
            \end{itemize}
        \end{column}
        \begin{column}{0.4\textwidth}
            \vspace{-1em}
            \begin{center}
            \begin{tikzpicture}[
                % Base font
                every node/.style={font=\fontsize{4}{5}\selectfont, inner sep=0.5pt},
                % Color legend:
                %   Orange = Control/scheduling (I-Cache, Scheduler, Dispatch)
                %   Blue   = Storage/caches (L0, Register File, L1 Data, Tex)
                %   Green  = Compute units (CUDA Cores, Tensor Cores, LD/ST)
                %   Cyan   = Tensor-specific (Tensor Memory, TMA)
                %   Yellow = Special functions (SFU)
                % Component styles with overlay-aware highlighting
                l0/.style={draw, minimum width=26mm, minimum height=3mm, align=center,
                    alt=<2->{fill=blue!40}{fill=gray!20}},
                sched/.style={draw, minimum width=26mm, minimum height=3mm, align=center,
                    alt=<1->{fill=orange!50}{fill=gray!20}},
                regf/.style={draw, minimum width=26mm, minimum height=3mm, align=center,
                    alt=<2->{fill=blue!60, text=white}{fill=gray!30}},
                tmem/.style={draw, minimum width=26mm, minimum height=3mm, align=center,
                    alt=<2->{fill=cyan!50}{fill=gray!20}},
                cuda/.style={draw, minimum width=12mm, minimum height=4mm, align=center,
                    alt=<1->{fill=green!60}{fill=gray!30}},
                tensor/.style={draw, minimum width=12mm, minimum height=4mm, align=center,
                    alt=<1->{fill=green!60}{fill=gray!30}},
                ldst/.style={draw, minimum width=3mm, minimum height=2.5mm, font=\fontsize{3}{4}\selectfont,
                    alt=<1->{fill=green!70}{fill=gray!30}},
                sfu/.style={draw, minimum width=3mm, minimum height=2.5mm, font=\fontsize{3}{4}\selectfont,
                    alt=<1->{fill=yellow!60}{fill=gray!20}},
                wide/.style={minimum width=56mm, minimum height=3.5mm, align=center},
                tex/.style={draw, minimum width=13mm, minimum height=3.5mm, align=center,
                    alt=<2->{fill=blue!40}{fill=gray!20}},
                % Processing block as reusable pic
                pics/procblock/.style={code={
                    \node[l0, anchor=north] (-l0) at (0,0) {L0 Instruction Cache};
                    \node[sched, anchor=north] (-ws) at (-l0.south) {Warp Scheduler (32 thd/clk)};
                    \node[sched, anchor=north] (-du) at (-ws.south) {Dispatch Unit (32 thd/clk)};
                    \node[regf, anchor=north] (-rf) at (-du.south) {Register File (16K$\times$32b)};
                    \node[tmem, anchor=north] (-tm) at (-rf.south) {64KB Tensor Memory};
                    \node[cuda, anchor=north west] (-cc) at (-tm.south west) {CUDA\\Cores};
                    \node[tensor, anchor=north east] (-tc) at (-tm.south east) {Tensor\\Cores};
                    \node[ldst, minimum width=20mm, anchor=north west] (-ld) at (-cc.south west) {LD/ST $\times$8};
                    \node[sfu, anchor=north east] (-sf) at (-tc.south east) {SFU};
                    % Border around the block
                    \node[draw, thick, inner sep=0.5pt, fit=(-l0) (-ld) (-sf)] {};
                }},
            ]
                % Title
                \node[font=\fontsize{5}{6}\selectfont\bfseries] (title) {Streaming Multiprocessor (SM)};

                % L1 Instruction Cache - revealed with memory (stage 2)
                \node[draw, wide, anchor=north, alt=<2->{fill=orange!40}{fill=gray!20}] (l1i) at ([yshift=-1mm]title.south) {L1 Instruction Cache};

                % Top row of processing blocks - relative to L1
                \pic (TL) at ([xshift=-14mm, yshift=-1mm]l1i.south) {procblock};
                \pic (TR) at ([xshift=14mm, yshift=-1mm]l1i.south) {procblock};

                % Bottom row - relative to top row
                \pic (BL) at ([yshift=-2mm]TL-ld.south -| TL-l0) {procblock};
                \pic (BR) at ([yshift=-2mm]TR-ld.south -| TR-l0) {procblock};

                % Bottom section - relative to bottom blocks
                \node[draw, wide, anchor=north, alt=<2->{fill=cyan!40}{fill=gray!20}] (tma) at ([yshift=-2mm]BL-ld.south -| title) {Tensor Memory Accelerator};
                \node[draw, wide, anchor=north, alt=<2->{fill=blue!30}{fill=gray!20}] (l1d) at ([yshift=-0.5mm]tma.south) {256KB L1 / Shared Mem};

                % Tex units as matrix - relative to L1 data
                \matrix[matrix of nodes, column sep=1mm, anchor=north, nodes={tex}, ampersand replacement=\&] (texunits) at ([yshift=-0.5mm]l1d.south) {
                    Tex \& Tex \& Tex \& Tex \\
                };

                % Outer SM box - fit around all content
                \begin{scope}[on background layer]
                    \node[draw, thick, fill=gray!20, fit=(title) (l1i) (TL-l0) (TR-l0) (BL-ld) (BR-sf) (texunits), inner sep=2mm] (sm) {};
                \end{scope}
            \end{tikzpicture}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{CUDA Compilation}
    \begin{center}
    \begin{tikzpicture}[
        node distance=0.6cm and 1.2cm,
        box/.style={draw, rounded corners, minimum width=28mm, minimum height=8mm, font=\small, thick},
        source/.style={box, fill=blue!20, draw=blue!60},
        compiler/.style={box, fill=green!20, draw=green!60},
        ir/.style={box, fill=yellow!25, draw=yellow!70!black},
        output/.style={box, fill=orange!25, draw=orange!60},
        arrow/.style={->, thick, >=stealth},
    ]
        % Vertical flow
        \node[source] (src) {\texttt{.cu} Source Code};
        \node[compiler, below=of src] (nvcc) {\textbf{nvcc} (NVIDIA Compiler)};
        \node[ir, below=of nvcc] (ptx) {PTX Code (IR)};
        % Two branches from PTX
        \node[compiler, below left=of ptx] (offline) {Offline Compiler};
        \node[compiler, below right=of ptx] (jit) {JIT Compiler};
        % Final output
        \node[output, below=1.0cm of ptx] (machine) {GPU Machine Code};

        % Labels for branches
        \node[font=\scriptsize, gray, left=0.1cm of offline] {build time};
        \node[font=\scriptsize, gray, right=0.1cm of jit] {runtime};

        \draw[arrow, blue!60] (src) -- (nvcc);
        \draw[arrow, green!60!black] (nvcc) -- (ptx);
        \draw[arrow, green!60!black] (ptx) -- (offline);
        \draw[arrow, green!60!black] (ptx) -- (jit);
        \draw[arrow, orange!70!black] (offline) -- (machine);
        \draw[arrow, orange!70!black] (jit) -- (machine);
    \end{tikzpicture}
    \end{center}

    \begin{itemize}
        \item PTX: Intermediate representation (resembles GPU instruction set)
        \item GPU peculiarities hidden from programmer
        \item Use PTX intrinsics for performance
    \end{itemize}
\end{frame}

%===============================================================================
\section{GPU Compute}
%===============================================================================

\begin{frame}{CUDA Execution Model}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Hierarchy (software $\rightarrow$ hardware):}
            \begin{description}
                \item[Thread] Single execution unit; has private registers
                \item[Warp] 32 threads executing in lockstep (SIMT)
                \item[Thread Block] Group of threads that can cooperate via shared memory; mapped to one SM
                \item[Grid] Collection of blocks executing same kernel
            \end{description}

            \vspace{0.5em}
            \textbf{Hardware Mapping:}
            \begin{description}
                \item[SM] Streaming Multiprocessor; executes one or more blocks concurrently
                \item[GPU] Contains many SMs (e.g., 148 on B200)
            \end{description}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
            \begin{tikzpicture}[
                % Node styles
                box/.style={draw, rounded corners, minimum width=22mm, minimum height=7mm, font=\footnotesize},
                sw/.style={box, fill=blue!20},
                hw/.style={box, fill=orange!20},
                header/.style={font=\scriptsize\bfseries},
                % Arrow styles
                hierarchy/.style={->, thick},
                mapping/.style={->, thick, dashed, gray},
            ]
                % Matrix layout
                \matrix[matrix of nodes,
                        ampersand replacement=\&,
                        column sep=12mm,
                        row sep=6mm,
                        nodes={anchor=center}] (m) {
                    |[header, blue!70!black]| Software \& |[header, orange!70!black]| Hardware \\
                    |[sw]| Grid               \& |[hw]| GPU \\
                    |[sw]| Thread Block       \& |[hw]| SM \\
                    |[sw]| Warp (32 threads)  \& |[hw]| CUDA Cores \\
                    |[sw]| Thread             \& |[hw]| Registers \\
                };

                % Hierarchy arrows (vertical)
                \foreach \i/\j in {2/3, 3/4, 4/5} {
                    \draw[hierarchy] (m-\i-1) -- (m-\j-1);
                    \draw[hierarchy] (m-\i-2) -- (m-\j-2);
                }

                % Mapping arrows (horizontal)
                \foreach \i in {2, 3, 5} {
                    \draw[mapping] (m-\i-1) -- (m-\i-2);
                }
            \end{tikzpicture}
            \end{center}
        \end{column}
    \end{columns}

    \vspace{0.3em}
    {\scriptsize Source: CUDA C++ Programming Guide}
\end{frame}

\begin{frame}[fragile]{SM Architecture \& Warp Scheduling}
\begin{center}
\begin{tikzpicture}[scale=0.85, every node/.style={font=\tiny, transform shape},
    arrow/.style={-stealth},
    revarrow/.style={stealth-},
    doublearrow/.style={stealth-stealth},
    box/.style={draw, fill=#1, minimum height=7mm, align=center},
    box/.default=white,
    lane/.style={draw, fill=red!10, minimum width=6mm, minimum height=5mm, align=center, font=\fontsize{4}{5}\selectfont},
    mux/.style={muxdemux, muxdemux def={Lh=0.5, Rh=0.25, NL=2, NR=1, w=0.4, inset w=0.15, inset Lh=0.25, inset Rh=0},
                external pins width=0, fill=white, rotate=-90, anchor=lpin 1},
    lsu/.style={muxdemux, muxdemux def={Lh=1.2, Rh=1.2, NL=0, NR=0, NB=2, w=1.1},
                external pins width=0, fill=violet!10, font=\fontsize{4}{5}\selectfont},
    reg/.style={muxdemux, muxdemux def={Lh=1.2, Rh=1.2, NL=0, NR=0, NB=1, NT=3, w=1.1},
                external pins width=0, fill=orange!15, font=\fontsize{4}{5}\selectfont},
    ireg/.style={muxdemux, muxdemux def={Lh=0.6, Rh=0.6, NL=0, NR=0, NB=16, NT=1, w=18},
                external pins width=0, fill=green!10, font=\scriptsize},
    connector/.style={fill=black, circle, minimum size=1mm, inner sep=0pt},
    memunit/.style={muxdemux, muxdemux def={Lh=0.8, Rh=0.8, NL=0, NR=0, NT=#1, NB=2, w=7},
                external pins width=0, fill=cyan!15, font=\scriptsize},
    memunit/.default=1,
]

% Scoreboard table as matrix of nodes
\matrix[matrix of nodes,
    nodes={font=\fontsize{6}{7}\selectfont, inner sep=1.5pt, anchor=center, text depth=0.5ex},
    column 1/.style={nodes={minimum width=8mm}},
    column 2/.style={nodes={minimum width=8mm}},
    column 3/.style={nodes={minimum width=18mm, anchor=west}},
    column 4/.style={nodes={minimum width=10mm}},
    row 1/.style={nodes={font=\fontsize{6}{7}\selectfont\bfseries, fill=gray!20}},
    row sep=0pt, column sep=2pt,
    draw, inner sep=1pt,
    label={[font=\scriptsize]above:Scoreboard},
] (scoretable) {
    Warp No. & Address & SIMD instructions & Operands? \\
    1 & 42 & ld.global.f64 & Ready \\
    1 & 43 & mul.f64 & No \\
    3 & 95 & shl.s32 & Ready \\
    3 & 96 & add.s32 & No \\
    8 & 11 & ld.global.f64 & Ready \\
    8 & 12 & ld.global.f64 & Ready \\
};
% Draw horizontal lines
\draw (scoretable.north west) -- (scoretable.north east);
\draw (scoretable-1-1.south west) -- (scoretable-1-4.south east);
\draw (scoretable.south west) -- (scoretable.south east);
% Dotted lines between data rows
\foreach \row in {2,...,6} {
    \draw[dotted] (scoretable-\row-1.south west) -- (scoretable-\row-4.south east);
}
% Vertical lines (use consistent x from header row)
\draw (scoretable.north west) -- (scoretable.south west);
\draw (scoretable.north east) -- (scoretable.south east);
\foreach \col in {1,2,3} {
    \draw (scoretable-1-\col.north east |- scoretable.north) -- (scoretable-1-\col.south east |- scoretable.south);
}
\node[font=\fontsize{6}{7}\selectfont, anchor=south west] at (scoretable.north west) (schedulertitle) {Warp scheduler};
\begin{scope}[on background layer]
\node[draw, fill=yellow!10, fit=(scoretable)(schedulertitle), inner sep=1pt] (scoreboard) {};
\end{scope}

% Instruction cache (positioned left of scoreboard) - blue for fetch
\node[box=blue!15, minimum width=16mm, minimum height=10mm, anchor=east, font=\scriptsize] (icache) at ([xshift=-4mm]scoreboard.west) {Instruction\\cache};

% Arrow from instruction cache to scoreboard
\draw[arrow, thick] (icache.east) -- (scoreboard.west);

% Instruction register as muxdemux
\node[ireg, anchor=north]
    (ireg) at ([yshift=-4mm]scoreboard.south) {Instruction register};

% Arrow from SIMD instructions column to instruction register
\draw[arrow, thick] (scoretable-7-3.south) -- ++(0, -0.3) -| (ireg.tpin 1);

% Arrow from address to instruction cache
\draw[arrow, thick] (scoretable-7-2.south) -- ++(0, -0.3) -| (icache.south);

% Draw 16 SIMD lanes below instruction register
\foreach \i in {1,...,16} {
    % ALU - first, below ireg bpin
    \node[lane, anchor=north, minimum height=7mm] at ([yshift=-2mm]ireg.bpin \i) (alu\i) {};

    % Registers box below ALU (modern GPUs: 64K regs per SM, ~4K per lane)
    \node[reg, anchor=north, align=center] at (alu\i.south) (reg\i) {Reg\\4K\\$\times$32};

    % Load store unit below Reg
    \node[lsu, anchor=north, align=center] at (reg\i.south) (lsu\i) {Load\\Store\\Unit};

    % Mux above Reg at tpin 2
    \node[mux, anchor=rpin 1] (mux\i) at ([yshift=2mm]reg\i.tpin 2) {};

    % Connect ireg bpin to ALU
    \draw[arrow, thin] (ireg.bpin \i) -- (alu\i.north);

    \draw[arrow, thin] (mux\i.rpin 1) -- (reg\i.tpin 2);

    \draw (reg\i.tpin 1) |- ([yshift=1mm]mux\i.lpin 1) -- (mux\i.lpin 1);

    \node[connector] (conn\i) at ([yshift=1mm]mux\i.lpin 2) {};
    \draw (conn\i) -- (mux\i.lpin 2);
}

% SIMD Lanes label with callout explaining real numbers
\node[anchor=west, font=\scriptsize, align=left] at ([xshift=2mm]mux16.east)
    (lanelabel) {SIMD Lanes\\(Thread\\Processors)};
\only<3>{
\node[rectangle callout, callout absolute pointer={(lanelabel.north)}, draw, fill=white,
    font=\footnotesize, align=left, anchor=south]
    at ([yshift=1.5cm]lanelabel.north) {Blackwell: 128 cores/SM\\$\div$ 32 threads/warp\\= 4 warps in parallel};
}

% Address coalescing unit - cyan for memory subsystem
\node[memunit=1, anchor=north]
    (coalesce) at ([yshift=-6mm]lsu5.bpin 1) {Address coalescing unit};

% Interconnection network - cyan for memory subsystem
\node[memunit=1] (interconnect) at (coalesce -| lsu12.bpin 1) {Interconnection network};

\foreach \i in {1,...,16} {
    \draw[arrow, thin] (lsu\i.bpin 1) -- ++(0,-0.4) coordinate (lsuconn\i) -| (coalesce.north);
    \node[connector] at (lsuconn\i) {};
}
% Horizontal bus connecting all LSU bpin 2 with connectors
\foreach \i in {1,...,16} {
    \draw[revarrow, thin] (lsu\i.bpin 2) -- ++(0,-0.2) coordinate (lsubus\i);
    \node[connector] at (lsubus\i) {};
}
\draw[thin] (lsubus1) -- (lsubus16);

\draw[arrow, thin] (lsu12.bpin 1) -- (interconnect.tpin 1);

% Local memory - cyan for memory subsystem (modern GPUs: 64-228KB configurable)
\node[memunit=2, fill=cyan!20, anchor=tpin 1]
    (localmem) at ([yshift=-6mm]coalesce.bpin 2) {Local Memory 96 KB};

% Arrow between coalescing and local memory
\draw[arrow, thick] (coalesce.bpin 2) -- (localmem.tpin 1);

% Arrow between local memory and interconnect
\draw[doublearrow, thick] (interconnect.bpin 1) -- (localmem.north -| interconnect.bpin 1);

% To global memory
\node[anchor=west, font=\scriptsize, align=center] at ([yshift=-3mm]interconnect.south east) (gmem)
    {To Global\\Memory};
\draw[arrow, thick] (interconnect.bpin 2) |- ([yshift=2mm]gmem.west);

% Draw connector from the line of address coalescing unit to global memory
\node[connector] at (coalesce.bpin 2 |- gmem) {};
\draw[arrow, thick] (coalesce.bpin 2 |- gmem) -- (gmem.west);

% Callout explaining multi-warp scheduling (slide 2 only, right side)
\only<2>{
\node[rectangle callout, callout absolute pointer={(scoretable.east)}, draw, fill=white,
    font=\footnotesize, align=left, anchor=west]
    at ([xshift=5mm]scoretable.east) {4 warp schedulers:\\issue 4 warps/cycle\\+ dual-issue per warp};
}

\end{tikzpicture}
\end{center}
\end{frame}
\begin{frame}{GPU Scheduler}
    \textbf{Grid \& Block Dispatch:}
    \begin{itemize}
        \item Kernel $\rightarrow$ grid of blocks
        \item SMs pull blocks when resources available
    \end{itemize}

    \vspace{0.5em}
    \textbf{Warp Scheduling:}
    \begin{itemize}
        \item Threads grouped into 32-thread \textbf{warps}
        \item Warp scheduler issues ready warps to hide stalls
        \item \textbf{Occupancy} = $\frac{\text{active warps}}{\text{max warps per SM}}$
    \end{itemize}

    \vspace{0.5em}
    \textbf{Latency Hiding:}
    \begin{itemize}
        \item High occupancy $\Rightarrow$ many warps queued
        \item When one warp stalls (memory), another executes
    \end{itemize}

    \vspace{0.5em}
    \textbf{Tune for Balance:}
    \begin{itemize}
        \item Choose block size to maximize occupancy
        \item Limit registers/thread to avoid spills
    \end{itemize}
\end{frame}

\begin{frame}{Registers}
    \textbf{Limited On-Chip Resource:}
    \begin{itemize}
        \item Each SM has fixed register file (e.g., 64K registers)
        \item Each thread can use at most 255 registers
    \end{itemize}

    \vspace{0.5em}
    \textbf{Trade-off:}
    \begin{itemize}
        \item Registers per thread cap number of resident warps
        \item High occupancy helps hide memory latency
        \item Excessive register use improves ILP but may starve SM
    \end{itemize}

    \vspace{0.5em}
    \textbf{Tuning:}
    \begin{itemize}
        \item Use \texttt{-maxrregcount} or \texttt{\_\_launch\_bounds\_\_()} to limit registers
        \item \textbf{Register spilling}: 100s of cycles penalty
        \item Refactor code to reuse variables and reduce live ranges
    \end{itemize}
\end{frame}

\begin{frame}{GPU Warp Divergence}
    \textbf{Problem:} Conditional branches cause threads within a warp to diverge.

    \begin{itemize}
        \item All 32 threads in a warp share \textbf{one Program Counter}
        \item When threads take different paths, execution is \textbf{serialized}
        \item Hardware uses an \textbf{active mask} to track which threads execute
        \item PC and mask are pushed onto a \textbf{stack} at each branch
    \end{itemize}

    \vspace{1em}
    \textbf{Convergence:} When both paths complete, threads merge back and all become active again.

    \vspace{1em}
    \textbf{Performance Impact:}
    \begin{itemize}
        \item Divergent warps take time proportional to \textbf{sum of all paths}
        \item Worst case: 32$\times$ slowdown if every thread takes a different path
        \item Nested branches compound the problem
    \end{itemize}

    \vspace{1em}
    \textbf{Mitigation strategies:} Data reorganization, predicated instructions, separate kernels, warp-level primitives
\end{frame}

\begin{frame}[fragile]{GPU Warp Divergence: Example}
\begin{columns}
\begin{column}{0.45\textwidth}
\begin{lstlisting}[language=C, basicstyle=\ttfamily\small, numbers=left, numberstyle=\tiny, xleftmargin=2em, backgroundcolor={}, escapechar=|]
int tid = threadIdx.x;
if (tid < 4) {
    |\tikzmark{L3s}|foo();|\tikzmark{L3e}|
    if (tid < 2) {
        |\tikzmark{L5s}|bar();|\tikzmark{L5e}|
    } else {
        |\tikzmark{L7s}|baz();|\tikzmark{L7e}|
    }
} else {
    |\tikzmark{L10s}|qux();|\tikzmark{L10e}|
}
|\tikzmark{L12s}|// All converge|\tikzmark{L12e}|
\end{lstlisting}
\end{column}

\begin{column}{0.5\textwidth}
\begin{tikzpicture}[scale=0.92,
    active/.style={draw, fill=green!40, minimum width=5mm, minimum height=4.5mm, font=\scriptsize},
    inactive/.style={draw, fill=white, minimum width=5mm, minimum height=4.5mm, font=\scriptsize},
    steplbl/.style={font=\scriptsize\bfseries, anchor=south west, yshift=-3mm},
    dots/.style={font=\scriptsize, minimum width=5mm},
]

% Step 1 - All 32 Threads Active
\only<1->{
\matrix[matrix of nodes, row sep=0pt, column sep=1pt, anchor=north west, ampersand replacement=\&] (s1) at (0,0) {
    |[active]| 0 \& |[active]| 1 \& |[active]| 2 \& |[active]| 3 \&
    |[active]| 4 \& |[active]| 5 \& |[active]| 6 \& |[active]| 7 \& |[dots]| \ldots{} 31 \\
};
\node[steplbl] at (s1.north west) {Step 1: All 32 Threads Active};
}

% Step 2 - foo() for tid < 4
\only<2->{
\matrix[matrix of nodes, row sep=0pt, column sep=1pt, anchor=north west, ampersand replacement=\&] (s2) at (0,-1.1) {
    |[active]| 0 \& |[active]| 1 \& |[active]| 2 \& |[active]| 3 \&
    |[inactive]| 4 \& |[inactive]| 5 \& |[inactive]| 6 \& |[inactive]| 7 \& |[dots]| \ldots \\
};
\node[steplbl] at (s2.north west) {Step 2: foo()};
}

% Step 3 - bar() for tid < 2
\only<3->{
\matrix[matrix of nodes, row sep=0pt, column sep=1pt, anchor=north west, ampersand replacement=\&] (s3) at (0,-2.2) {
    |[active]| 0 \& |[active]| 1 \& |[inactive]| 2 \& |[inactive]| 3 \&
    |[inactive]| 4 \& |[inactive]| 5 \& |[inactive]| 6 \& |[inactive]| 7 \& |[dots]| \ldots \\
};
\node[steplbl] at (s3.north west) {Step 3: bar()};
}

% Step 4 - baz() for 2 <= tid < 4
\only<4->{
\matrix[matrix of nodes, row sep=0pt, column sep=1pt, anchor=north west, ampersand replacement=\&] (s4) at (0,-3.3) {
    |[inactive]| 0 \& |[inactive]| 1 \& |[active]| 2 \& |[active]| 3 \&
    |[inactive]| 4 \& |[inactive]| 5 \& |[inactive]| 6 \& |[inactive]| 7 \& |[dots]| \ldots \\
};
\node[steplbl] at (s4.north west) {Step 4: baz()};
}

% Step 5 - qux() for tid >= 4
\only<5->{
\matrix[matrix of nodes, row sep=0pt, column sep=1pt, anchor=north west, ampersand replacement=\&] (s5) at (0,-4.4) {
    |[inactive]| 0 \& |[inactive]| 1 \& |[inactive]| 2 \& |[inactive]| 3 \&
    |[active]| 4 \& |[active]| 5 \& |[active]| 6 \& |[active]| 7 \& |[dots]| \ldots \\
};
\node[steplbl] at (s5.north west) {Step 5: qux()};
}

% Step 6 - Convergence
\only<6>{
\matrix[matrix of nodes, row sep=0pt, column sep=1pt, anchor=north west, ampersand replacement=\&] (s6) at (0,-5.5) {
    |[active]| 0 \& |[active]| 1 \& |[active]| 2 \& |[active]| 3 \&
    |[active]| 4 \& |[active]| 5 \& |[active]| 6 \& |[active]| 7 \& |[dots]| \ldots{} 31 \\
};
\node[steplbl, text=green!50!black] at (s6.north west) {Step 6: Convergence};
}

% Legend
\only<1->{
\node[active, label=right:{\scriptsize Active}] at (1,-7.3) {};
\node[inactive, label=right:{\scriptsize Inactive (masked)}] at (3,-7.3) {};
}

\end{tikzpicture}
\end{column}
\end{columns}

% Code highlighting overlay using tikzmarks - yellow background with blend mode
\begin{tikzpicture}[remember picture, overlay]
    \begin{scope}[blend mode=multiply]
    % Step 2: highlight "foo();" - runs for all tid < 4
    \only<2>{
        \fill[yellow!60, rounded corners=2pt]
            ([xshift=-2pt, yshift=9pt]pic cs:L3s) rectangle
            ([xshift=2pt, yshift=-1pt]pic cs:L3e);
    }
    % Step 3: highlight "bar();" - runs for tid < 2
    \only<3>{
        \fill[yellow!60, rounded corners=2pt]
            ([xshift=-2pt, yshift=9pt]pic cs:L5s) rectangle
            ([xshift=2pt, yshift=-1pt]pic cs:L5e);
    }
    % Step 4: highlight "baz();" - runs for 2 <= tid < 4
    \only<4>{
        \fill[yellow!60, rounded corners=2pt]
            ([xshift=-2pt, yshift=9pt]pic cs:L7s) rectangle
            ([xshift=2pt, yshift=-1pt]pic cs:L7e);
    }
    % Step 5: highlight "qux();" - runs for tid >= 4
    \only<5>{
        \fill[yellow!60, rounded corners=2pt]
            ([xshift=-2pt, yshift=9pt]pic cs:L10s) rectangle
            ([xshift=2pt, yshift=-1pt]pic cs:L10e);
    }
    % Step 6: highlight "// All converge" - all threads reconverge
    \only<6>{
        \fill[green!60, rounded corners=2pt]
            ([xshift=-2pt, yshift=9pt]pic cs:L12s) rectangle
            ([xshift=2pt, yshift=-1pt]pic cs:L12e);
    }
    \end{scope}
\end{tikzpicture}
\end{frame}
\begin{frame}[fragile]{Warp-Level Primitives Example}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Warp reduction without divergence:}

\begin{lstlisting}[basicstyle=\ttfamily\small]
#define FULL_MASK 0xffffffff

for (int offset = 16; offset > 0; offset /= 2)
    val += __shfl_down_sync(FULL_MASK, val, offset);
\end{lstlisting}

            \textbf{Benefits:}
            \begin{itemize}
                \item No shared memory needed
                \item No synchronization barriers
                \item Efficient intra-warp communication
            \end{itemize}

            \textbf{Replace branches with arithmetic:}
\begin{lstlisting}[basicstyle=\ttfamily\small]
result = condition * trueValue
       + (!condition) * falseValue;
\end{lstlisting}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
            \begin{tikzpicture}[
                val/.style={draw, minimum width=6.5mm, minimum height=5mm,
                            font=\tiny\ttfamily, fill=green!20, inner sep=0pt},
                valg/.style={val, fill=gray!15, text=gray},
                hdr/.style={minimum width=6.5mm, minimum height=4mm,
                            font=\tiny\ttfamily, inner sep=0pt, text=blue!70!black},
                lbl/.style={font=\tiny\bfseries, text=black, minimum width=10mm,
                            align=right, anchor=east},
                gap/.style={minimum height=6mm, inner sep=0pt},
                arr/.style={->, thick, red!70!black},
            ]
                \matrix[matrix of nodes, column sep=0pt, row sep=0pt,
                        ampersand replacement=\&] (m) {
                    % Row 1: Lane headers
                    |[lbl]| Lane \&
                    |[hdr]| 0 \& |[hdr]| 1 \& |[hdr]| 2 \& |[hdr]| 3 \&
                    |[hdr]| 4 \& |[hdr]| 5 \& |[hdr]| 6 \& |[hdr]| 7 \\
                    % Row 2: Initial values
                    |[lbl]| val \&
                    |[val]| 1 \& |[val]| 2 \& |[val]| 3 \& |[val]| 4 \&
                    |[val]| 5 \& |[val]| 6 \& |[val]| 7 \& |[val]| 8 \\
                    % Row 3: Gap for arrows
                    |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \&
                    |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \\
                    % Row 4: After offset=4
                    |[lbl]| val \&
                    |[val]| 6 \& |[val]| 8 \& |[val]| 10 \& |[val]| 12 \&
                    |[valg]| 5 \& |[valg]| 6 \& |[valg]| 7 \& |[valg]| 8 \\
                    % Row 5: Gap for arrows
                    |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \&
                    |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \\
                    % Row 6: After offset=2
                    |[lbl]| val \&
                    |[val]| 16 \& |[val]| 20 \& |[valg]| 10 \& |[valg]| 12 \&
                    |[valg]| 5 \& |[valg]| 6 \& |[valg]| 7 \& |[valg]| 8 \\
                    % Row 7: Gap for arrows
                    |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \&
                    |[gap]| \& |[gap]| \& |[gap]| \& |[gap]| \\
                    % Row 8: Final result
                    |[lbl]| val \&
                    |[val, fill=green!40]| 36 \& |[valg]| 20 \& |[valg]| 10 \& |[valg]| 12 \&
                    |[valg]| 5 \& |[valg]| 6 \& |[valg]| 7 \& |[valg]| 8 \\
                };

                % Shuffle arrows: offset=4 (row 2 -> row 4)
                \foreach \src/\dst in {6/2, 7/3, 8/4, 9/5} {
                    \draw[arr] (m-2-\src.south) -- (m-4-\dst.north);
                }
                % Shuffle arrows: offset=2 (row 4 -> row 6)
                \foreach \src/\dst in {4/2, 5/3} {
                    \draw[arr] (m-4-\src.south) -- (m-6-\dst.north);
                }
                % Shuffle arrows: offset=1 (row 6 -> row 8)
                \draw[arr] (m-6-3.south) -- (m-8-2.north);

                % Operation annotations aligned to right edge of last value cell
                \node[font=\tiny\ttfamily, text=red!70!black, anchor=east, fill=white, inner sep=1pt]
                    at (m-2-9.east |- m-3-9) {\_\_shfl\_down\_sync(..., 4)};
                \node[font=\tiny\ttfamily, text=red!70!black, anchor=east, fill=white, inner sep=1pt]
                    at (m-2-9.east |- m-5-9) {\_\_shfl\_down\_sync(..., 2)};
                \node[font=\tiny\ttfamily, text=red!70!black, anchor=east, fill=white, inner sep=1pt]
                    at (m-2-9.east |- m-7-9) {\_\_shfl\_down\_sync(..., 1)};
            \end{tikzpicture}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Kernel Fusion and Fission}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Kernel Fusion:}
            \begin{itemize}
                \item Combine multiple sequential kernels into one
                \item Reduces launch overhead
                \item Reduces global memory traffic
                \item Reuse data in registers/shared memory
            \end{itemize}

            \textbf{Trade-off:}
            \begin{itemize}
                \item May increase register pressure
                \item Can limit occupancy
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Kernel Fission:}
            \begin{itemize}
                \item Split large kernel into smaller ones
                \item Lowers register/shared memory usage
                \item Improves occupancy
                \item Exposes finer-grained parallelism
            \end{itemize}

            \textbf{Trade-off:}
            \begin{itemize}
                \item Extra launches
                \item Additional memory transfers
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{DPX Instructions}
    \begin{columns}
        \begin{column}{0.40\textwidth}
            \textbf{Dynamic Programming eXtensions:}

            \begin{itemize}
                \item Specialized fused-compute instructions
                \item Find min/max values with fused addition
                \item Works on 16 and 32-bit signed/unsigned integers
                \item Optional ReLU (clamping to zero)
            \end{itemize}
        \end{column}
        \begin{column}{0.60\textwidth}
            \small\textbf{Example: Floyd--Warshall Algorithm}
            \begin{itemize}
                \item Find shortest paths: $\text{dist}[i][j] = \min(\text{dist}[i][j], \text{dist}[i][k] + \text{dist}[k][j])$
                \item \textbf{DPX accelerates the \texttt{min(a, b+c)} operation}
            \end{itemize}
            \vspace{-0.5em}
            \centering
            \begin{tikzpicture}[
                fwnode/.style={circle, draw, minimum size=1em, inner sep=0pt, font=\tiny\bfseries},
                fwbold/.style={fwnode, fill=black!10},
                arr/.style={->, >=stealth, shorten <=0.5pt, shorten >=0.5pt},
                lbl/.style={font=\tiny, midway, above=1mm, inner sep=0pt},
                row sep=0.2em,
                column sep=2.3mm,
                every matrix/.style={inner sep=0pt, ampersand replacement=\&},
                bluebox/.style={draw=blue!70, rounded corners=1pt, inner sep=1pt},
                redbox/.style={draw=red!70, rounded corners=1pt, inner sep=1pt},
            ]
            % === Left graph ===
            \matrix (G) [matrix of nodes, nodes={fwnode}, row sep=0.6em, column sep=1em] {
                   \& 1 \&   \\
                2  \&   \& 3 \\
                   \& 4 \&   \\
            };
            \draw[arr] (G-1-2) -- node[lbl, right, pos=0.3] {$\scalebox{0.7}{-2}$} (G-2-3);
            \draw[arr] (G-2-1) -- node[lbl, above left, pos=0.4] {$\scalebox{0.7}{4}$} (G-1-2);
            \draw[arr] (G-2-1) -- node[lbl, below] {$\scalebox{0.7}{3}$} (G-2-3);
            \draw[arr] (G-2-3) -- node[lbl, right, pos=0.5] {$\scalebox{0.7}{2}$} (G-3-2);
            \draw[arr] (G-3-2) -- node[lbl, left, pos=0.5] {$\scalebox{0.7}{-1}$} (G-2-1);

            % === k=0 column ===
            \matrix (K0) [matrix of nodes, nodes={fwnode}, right=1em of G, row sep=0.2em,
                label={[font=\tiny]above:$k\!=\!0$:}] {
                1 \& 3 \\
                2 \& 1 \\
                2 \& 3 \\
                3 \& 4 \\
                4 \& 2 \\
            };
            \foreach \r/\w in {1/-2, 2/4, 3/3, 4/2, 5/-1} {
                \draw[arr] (K0-\r-1) -- node[lbl] {$\scalebox{0.6}{\w}$} (K0-\r-2);
            }
            \node[bluebox, fit=(K0-5-1)(K0-5-2)] {};

            % === k=1 column ===
            \matrix (K1) [matrix of nodes, nodes={fwnode}, anchor=north west, row sep=0.2em,
                label={[font=\tiny]above:$k\!=\!1$:}] at ([xshift=3mm]K0.north east) {
                2 \& |[fwbold]| 1 \& 3 \\
            };
            \draw[arr] (K1-1-1) -- node[lbl] {$\scalebox{0.6}{4}$} (K1-1-2);
            \draw[arr] (K1-1-2) -- node[lbl] {$\scalebox{0.6}{-2}$} (K1-1-3);
            \node[redbox, fit=(K1-1-1)(K1-1-3)] {};

            % === k=2 column ===
            \matrix (K2) [matrix of nodes, nodes={fwnode}, anchor=north west, row sep=0.2em,
                label={[font=\tiny]above:$k\!=\!2$:}] at ([yshift=-1em]K1.south west) {
                4 \& |[fwbold]| 2 \& 1 \& |[draw=none]| \phantom{3} \\
                4 \& |[fwbold]| 2 \& 1 \& 3 \\
            };
            \draw[arr] (K2-1-1) -- node[lbl] {$\scalebox{0.6}{-1}$} (K2-1-2);
            \draw[arr] (K2-1-2) -- node[lbl] {$\scalebox{0.6}{4}$} (K2-1-3);
            \draw[arr] (K2-2-1) -- node[lbl] {$\scalebox{0.6}{-1}$} (K2-2-2);
            \draw[arr] (K2-2-2) -- node[lbl] {$\scalebox{0.6}{4}$} (K2-2-3);
            \draw[arr] (K2-2-3) -- node[lbl] {$\scalebox{0.6}{-2}$} (K2-2-4);
            \node[bluebox, fit=(K2-2-1)(K2-2-2)] {};
            \node[redbox, fit=(K2-2-2)(K2-2-4)] {};

            % === k=3 column ===
            \matrix (K3) [matrix of nodes, nodes={fwnode}, anchor=north west, row sep=0.2em,
                label={[font=\tiny]above:$k\!=\!3$:}] at ([yshift=-1em]K2.south west) {
                1 \& |[fwbold]| 3 \& 4 \& |[draw=none]| \phantom{3} \\
                2 \& 1 \& |[fwbold]| 3 \& 4 \\
            };
            \draw[arr] (K3-1-1) -- node[lbl] {$\scalebox{0.6}{-2}$} (K3-1-2);
            \draw[arr] (K3-1-2) -- node[lbl] {$\scalebox{0.6}{2}$} (K3-1-3);
            \draw[arr] (K3-2-1) -- node[lbl] {$\scalebox{0.6}{4}$} (K3-2-2);
            \draw[arr] (K3-2-2) -- node[lbl] {$\scalebox{0.6}{-2}$} (K3-2-3);
            \draw[arr] (K3-2-3) -- node[lbl] {$\scalebox{0.6}{2}$} (K3-2-4);

            % === k=4 column ===
            \matrix (K4) [matrix of nodes, nodes={fwnode}, anchor=north west, row sep=0.2em,
                label={[font=\tiny]above:$k\!=\!4$:}] at ([xshift=8mm]K1.north east) {
                3 \& |[fwbold]| 4 \& 2 \& |[draw=none]| \phantom{1} \\
                3 \& |[fwbold]| 4 \& 2 \& 1 \\
                1 \& 3 \& |[fwbold]| 4 \& 2 \\
            };
            \draw[arr] (K4-1-1) -- node[lbl] {$\scalebox{0.6}{2}$} (K4-1-2);
            \draw[arr] (K4-1-2) -- node[lbl] {$\scalebox{0.6}{-1}$} (K4-1-3);
            \draw[arr] (K4-2-1) -- node[lbl] {$\scalebox{0.6}{2}$} (K4-2-2);
            \draw[arr] (K4-2-2) -- node[lbl] {$\scalebox{0.6}{-1}$} (K4-2-3);
            \draw[arr] (K4-2-3) -- node[lbl] {$\scalebox{0.6}{4}$} (K4-2-4);
            \draw[arr] (K4-3-1) -- node[lbl] {$\scalebox{0.6}{-2}$} (K4-3-2);
            \draw[arr] (K4-3-2) -- node[lbl] {$\scalebox{0.6}{2}$} (K4-3-3);
            \draw[arr] (K4-3-3) -- node[lbl] {$\scalebox{0.6}{-1}$} (K4-3-4);
            \end{tikzpicture}
        \end{column}
    \end{columns}
\end{frame}

%===============================================================================
\section{Tensor Cores}
%===============================================================================

\begin{frame}{Tensor Cores}
    \begin{columns}
        \begin{column}{0.65\textwidth}
            \textbf{Specialized for matrix multiply-accumulate (MMA) operations.}

            \vspace{0.5em}
            \textbf{Tensor Cores dominate LLM workloads:}
            \begin{itemize}
                \item Transformers are \alert{matrix multiplication machines}
                \item Attention: $\text{softmax}\left(\frac{QK^T}{\sqrt{d}}\right) \cdot V$ --- all matmuls
                \item Feed-forward layers: dense matrix multiplications
                \item Single Tensor Core op: 16$\times$16$\times$16 MMA (FP16)
            \end{itemize}

            \vspace{0.5em}
            \textbf{WMMA (Warp Matrix Multiply Accumulate):}
            \begin{itemize}
                \item Used internally by cuBLAS, cuDNN, PyTorch, TensorFlow
                \item Fused multiply-add in single clock cycle
                \item GPU tiling: large matrices $\rightarrow$ small tiles in shared memory
            \end{itemize}

            \vspace{0.3em}
            \begin{block}{Key Insight}
                $>$90\% of LLM compute time is spent in Tensor Core operations.
            \end{block}
        \end{column}
        \begin{column}{0.35\textwidth}
            \vspace{-4.5em}
            \begin{center}
            \begin{tikzpicture}[
                cell/.style={minimum size=6mm, anchor=center, font=\footnotesize},
                arr/.style={->, thick, red!70!black}]
                % Row-major
                \matrix[matrix of math nodes, nodes={cell}, row sep=1pt,
                        column sep=1pt, ampersand replacement=\&,
                        left delimiter={[}, right delimiter={]},
                        label={[font=\scriptsize\bfseries]above:Row-major}] (R) {
                    a_{11} \& a_{12} \& a_{13} \\
                    a_{21} \& a_{22} \& a_{23} \\
                    a_{31} \& a_{32} \& a_{33} \\
                };
                \draw[arr, dashed] (R-1-1) -- (R-1-3) -- (R-2-1) -- (R-2-3) -- (R-3-1) -- (R-3-3);

                % Column-major
                \matrix[matrix of math nodes, nodes={cell}, row sep=1pt,
                        column sep=1pt, ampersand replacement=\&,
                        left delimiter={[}, right delimiter={]}, below=35pt of R,
                        label={[font=\scriptsize\bfseries]above:Column-major}] (C) {
                    a_{11} \& a_{12} \& a_{13} \\
                    a_{21} \& a_{22} \& a_{23} \\
                    a_{31} \& a_{32} \& a_{33} \\
                };
                \draw[arr, cyan!70!black, dashed] (C-1-1) -- (C-3-1) -- (C-1-2) -- (C-3-2) -- (C-1-3) -- (C-3-3);
            \end{tikzpicture}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}[fragile]{Tensor Cores: WMMA Example}
\begin{lstlisting}
#include <mma.h>
using namespace nvcuda;

__global__ void wmma_ker(half *a, half *b, float *c) {
   // Declare the fragments
   wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::col_major> a_frag;
   wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> b_frag;
   wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;

   // Initialize the output to zero
   wmma::fill_fragment(c_frag, 0.0f);

   // Load the inputs
   wmma::load_matrix_sync(a_frag, a, 16);
   wmma::load_matrix_sync(b_frag, b, 16);

   // Perform the matrix multiplication
   wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);

   // Store the output
   wmma::store_matrix_sync(c, c_frag, 16, wmma::mem_row_major);
}
\end{lstlisting}
\end{frame}

\begin{frame}{Tensor Core Precisions}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Floating-Point Formats:}
            \begin{description}
                \item[FP32] 32 bits --- full precision baseline
                \item[TF32] 19 bits --- $\sim$10$\times$ speedup, same range as FP32
                \item[FP16] 16 bits --- standard for deep learning
                \item[BF16] 16 bits --- same range as FP32, fewer mantissa bits
                \item[FP8] 8 bits --- Hopper+, 2$\times$ throughput vs FP16
                \item[FP4] 4 bits --- Blackwell, 4$\times$ throughput vs FP8
            \end{description}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Integer Formats:}
            \begin{description}
                \item[INT8] Quantized inference, 4$\times$ vs FP32
                \item[INT4] Highest throughput for extreme quantization
            \end{description}

            \vspace{1em}
            \textbf{Mixed Precision Pattern:}
            \begin{itemize}
                \item Inputs: FP16/BF16/FP8/FP4
                \item Accumulator: \alert{always FP32}
                \item Maintains numerical stability
            \end{itemize}

            \vspace{0.5em}
            \begin{block}{Trend}
                Each generation adds lower precision $\rightarrow$ higher throughput.
            \end{block}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Tensor Memory (TMEM) --- Blackwell}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{What is TMEM?}
            \begin{itemize}
                \item 256KB dedicated on-chip memory per SM
                \item 128 lanes $\times$ 512 columns $\times$ 4 bytes
                \item Specialized for Tensor Core accumulators
                \item Separate from registers and shared memory
            \end{itemize}

            \vspace{0.5em}
            \textbf{Benefits:}
            \begin{itemize}
                \item Zero register pressure for MMA operations
                \item Decouples Tensor Cores from CUDA Cores
                \item Higher occupancy possible
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Evolution of Tensor Core Data Flow:}
            \begin{enumerate}
                \item \textbf{Volta/Ampere:} Operands \& accumulators in registers
                \item \textbf{Hopper:} TMA loads operands directly to shared memory
                \item \textbf{Blackwell:} TMEM holds accumulators
            \end{enumerate}

            \vspace{0.5em}
            \textbf{Trade-offs:}
            \begin{itemize}
                \item Must explicitly manage TMEM allocation
                \item Custom ops on accumulators require TMEM $\leftrightarrow$ register copies
                \item Restricted access patterns (warpgroup-based)
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}<0>{Quantization}  % Hidden slide
    \textbf{Software technique: converting high-precision weights to lower-precision formats.}

    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Benefits:}
            \begin{itemize}
                \item Lower memory footprint
                \item Faster inference (Tensor Cores accelerate INT8/INT4/FP8/FP4)
                \item Enables deployment on resource-constrained hardware
            \end{itemize}

            \vspace{0.5em}
            \textbf{Common Schemes:}
            \begin{description}
                \item[PTQ] Post-Training Quantization --- quantize after training
                \item[QAT] Quantization-Aware Training --- simulate quantization during training
            \end{description}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Connection to Hardware:}
            \begin{itemize}
                \item FP32 model $\rightarrow$ INT8 weights = 4$\times$ smaller
                \item Tensor Cores have dedicated INT8/INT4 datapaths
                \item Mixed precision: INT8 compute, FP32 accumulator
            \end{itemize}

            \vspace{0.5em}
            \begin{alertblock}{Trade-off}
                Potential accuracy loss --- must validate on your workload.
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

%===============================================================================
\section{GPU Memory}
%===============================================================================

\begin{frame}{GPU Memory Hierarchy}
    \begin{columns}
        \begin{column}{0.45\textwidth}
            \textbf{B200 Memory Hierarchy:}
            \begin{center}
            \begin{tikzpicture}[
                level/.style={draw, minimum width=32mm, minimum height=7mm, font=\scriptsize, align=center},
                arr/.style={->, thick},
            ]
                \node[level, fill=red!30] (reg) {Registers\\256KB/SM, $\sim$1 cycle};
                \node[level, fill=orange!30, below=2mm of reg] (smem) {Shared Mem / L1\\256KB/SM, $\sim$30 cycles};
                \node[level, fill=yellow!30, below=2mm of smem] (l2) {L2 Cache\\128MB total, $\sim$200 cycles};
                \node[level, fill=green!30, below=2mm of l2] (hbm) {HBM3e (Global)\\192GB, $\sim$400 cycles};

                \draw[arr] (reg) -- (smem);
                \draw[arr] (smem) -- (l2);
                \draw[arr] (l2) -- (hbm);

                % Scope labels
                \node[font=\tiny, right=1mm of reg, text=gray] {per-thread};
                \node[font=\tiny, right=1mm of smem, text=gray] {per-SM};
                \node[font=\tiny, right=1mm of l2, text=gray] {all SMs};
                \node[font=\tiny, right=1mm of hbm, text=gray] {all SMs};
            \end{tikzpicture}
            \end{center}
        \end{column}
        \begin{column}{0.55\textwidth}
            \textbf{Shared Memory vs L1 Cache:}
            \begin{itemize}
                \item \alert{Same physical SRAM pool} (256KB per SM)
                \item \textbf{Shared Memory}: Programmer-managed scratchpad
                \item \textbf{L1 Cache}: Hardware-managed, caches global memory
                \item Configurable split via \texttt{cudaFuncSetAttribute()}
            \end{itemize}

            \vspace{0.5em}
            \textbf{L2 Cache:}
            \begin{itemize}
                \item Shared across all SMs (unlike L1)
                \item Caches global memory accesses automatically
                \item B200: 64MB per die $\times$ 2 dies = 128MB total
                \item Can pin data with L2 residency controls
            \end{itemize}

            \vspace{0.5em}
            \textbf{Key Insight:} Moving data closer to compute is critical for performance.
        \end{column}
    \end{columns}

    \vspace{0.3em}
    {\scriptsize Source: NVIDIA Blackwell Tuning Guide}
\end{frame}

\begin{frame}{HBM (High Bandwidth Memory)}
    \begin{columns}
        \begin{column}{0.55\textwidth}
            \textbf{3D-stacked DRAM architecture:}
            \begin{itemize}
                \item Memory dies stacked vertically atop base logic die
                \item Interconnected via through-silicon vias (TSVs)
            \end{itemize}

            \vspace{0.5em}
            \textbf{Benefits:}
            \begin{itemize}
                \item \textbf{Massive bandwidth}: 8 TB/s on B200
                \item \textbf{Low latency}: Short signal paths
                \item \textbf{Wide interface}: 8192-bit bus
                \item \textbf{Low power per bit}: Compared to GDDR
            \end{itemize}

            \vspace{0.5em}
            \textbf{Example:} B200 has 192GB HBM3e @ 8 TB/s
        \end{column}
        \begin{column}{0.45\textwidth}
            \begin{center}
                \includegraphics[width=\textwidth]{figures/10-hbm2e-architecture.jpeg}
            \end{center}
        \end{column}
    \end{columns}

    \vspace{0.3em}
    {\scriptsize Source: NVIDIA B200 Datasheet}
\end{frame}

\begin{frame}[fragile]{Memory Coalescing}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{How GPU memory works:}
            \begin{itemize}
                \item Memory accessed in \textbf{32-byte sectors}
                \item L1 cache line: 128 bytes (4 sectors)
                \item Warp (32 threads) issues one memory transaction
                \item \alert{Goal:} Minimize transactions per warp
            \end{itemize}

            \vspace{0.5em}
            \textbf{Coalesced access:}
            \begin{itemize}
                \item Adjacent threads access adjacent addresses
                \item 32 threads $\times$ 4 bytes = 128 bytes = 1 transaction
            \end{itemize}

            \vspace{0.5em}
            \textbf{Strided access:}
            \begin{itemize}
                \item Threads access non-contiguous addresses
                \item Stride of $N$: up to $N\times$ more transactions
                \item 10--30$\times$ performance penalty
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
\begin{lstlisting}[title={\color{green!50!black}\textbf{Coalesced (row-major)}}]
// Thread i accesses A[row][i]
float val = A[row * N + threadIdx.x];
\end{lstlisting}

\begin{lstlisting}[title={\color{red!70!black}\textbf{Non-coalesced (column-major)}}]
// Thread i accesses A[i][col]
float val = A[threadIdx.x * N + col];
\end{lstlisting}

            \vspace{0.5em}
            \begin{center}
            \begin{tikzpicture}[
                cell/.style={draw, minimum width=4mm, minimum height=4mm,
                             font=\tiny, inner sep=0pt},
                good/.style={cell, fill=green!25},
                bad/.style={cell, fill=red!25},
                lbl/.style={font=\tiny\bfseries},
            ]
                % Coalesced access
                \node[lbl] at (-0.8, 0) {Coalesced:};
                \foreach \i in {0,...,7} {
                    \node[good] at (\i*0.45, 0) {\i};
                }
                \draw[->, thick, green!50!black] (0, -0.4) -- (3.15, -0.4);
                \node[font=\tiny, green!50!black] at (1.6, -0.65) {1 transaction};

                % Strided access
                \node[lbl] at (-0.8, -1.4) {Strided:};
                \foreach \i in {0,...,7} {
                    \node[bad] at (\i*0.45, -1.4) {\i};
                }
                \foreach \i in {0,2,4,6} {
                    \draw[->, thick, red!70!black] (\i*0.45, -1.1) -- (\i*0.45, -1.2);
                }
                \node[font=\tiny, red!70!black] at (1.6, -1.95) {4 transactions};
            \end{tikzpicture}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Asynchronous Memory Copying}
    \textbf{Host $\rightarrow$ Device DMA:}
    \begin{itemize}
        \item GPU DMA engines transfer over PCIe/NVLink without CPU
        \item \texttt{cudaMemcpyAsync()} for non-blocking transfers
        \item Requires page-locked (pinned) host buffers
    \end{itemize}

    \vspace{0.5em}
    \textbf{Device-Side Global $\rightarrow$ Shared:}
    \begin{itemize}
        \item \texttt{cp.async} or \texttt{cuda::memcpy\_async}
        \item Data movement decoupled from warp execution
    \end{itemize}

    \vspace{0.5em}
    \textbf{Pipelining \& Overlap:}
    \begin{itemize}
        \item Multi-stage pipelines (double-buffering)
        \item Issue next-tile copies before consuming current
        \item Hides hundreds of cycles of global-memory latency
    \end{itemize}

    \vspace{0.5em}
    \textbf{Best Practices:}
    \begin{itemize}
        \item Use \texttt{cudaHostAlloc} for true async DMA
        \item Align data to cache-line boundaries
    \end{itemize}
\end{frame}

\begin{frame}[fragile]{Pipelining with Latency Hiding (1/2)}
\begin{lstlisting}[escapeinside={(*@}{@*)}]
template <size_t stages_count = 2 (*@\tikzmark{stages}@*)/* Pipeline with stages_count stages */>
__global__ void with_staging_unified(int* global_out, int const* global_in,
                                     size_t size, size_t batch_sz) {
    auto grid = cooperative_groups::this_grid();
    auto block = cooperative_groups::this_thread_block();
    assert(size == batch_sz * grid.size()); // Assume input size fits batch_sz * grid_size

    extern __shared__ int shared[];(*@\tikzmark{shared}@*) // stages_count * block.size() * sizeof(int) bytes
    size_t shared_offset[stages_count];
    for (int s = 0; s < stages_count; ++s) shared_offset[s] = s * block.size();

    __shared__ cuda::pipeline_shared_state<(*@\tikzmark{pipestate}@*)
        cuda::thread_scope::thread_scope_block,
        stages_count
    > shared_state;
    auto pipeline = cuda::make_pipeline(block, &shared_state);(*@\tikzmark{makepipe}@*)

    auto block_batch = [&](size_t batch) -> int {
        return block.group_index().x * block.size() + grid.size() * batch;
    };
\end{lstlisting}
\begin{tikzpicture}[remember picture, overlay,
    callout/.style={rectangle callout, draw=blue!70, fill=blue!10,
                    font=\tiny, callout absolute pointer={#1},
                    rounded corners=2pt, text width=22mm, align=center}]
    \only<2->{
    \node[callout={(pic cs:stages)}, anchor=south] at ([shift={(0mm,3mm)}]pic cs:stages)
        {Double buffering\\(2 stages)};
    \node[callout={(pic cs:shared)}, anchor=west] at ([shift={(50mm,0mm)}]pic cs:shared)
        {Staging buffers\\in shared memory};
    \node[callout={(pic cs:pipestate)}, anchor=west] at ([shift={(50mm,0mm)}]pic cs:pipestate)
        {Pipeline state for\\block synchronization};
    }
\end{tikzpicture}
\end{frame}

\begin{frame}[fragile]{Pipelining with Latency Hiding (2/2)}
\begin{lstlisting}[escapeinside={(*@}{@*)}]
    // compute_batch: next batch to process
    // fetch_batch:  next batch to fetch from global memory
    for (size_t compute_batch = 0, fetch_batch = 0;(*@\tikzmark{twoptr}@*) compute_batch < batch_sz;
         ++compute_batch) {
        // The outer loop iterates over the computation of the batches
        for (; fetch_batch < batch_sz && fetch_batch < (compute_batch + stages_count);(*@\tikzmark{keepfull}@*)
             ++fetch_batch) {
            // This inner loop iterates over the memory transfers,
            // making sure that the pipeline is always full
            pipeline.producer_acquire();(*@\tikzmark{produce}@*)
            size_t shared_idx = fetch_batch % stages_count;(*@\tikzmark{modulo}@*)
            size_t batch_idx = fetch_batch;
            size_t block_batch_idx = block_batch(batch_idx);
            cuda::memcpy_async(block, shared + shared_offset[shared_idx],(*@\tikzmark{async}@*)
                               global_in + block_batch_idx,
                               sizeof(int) * block.size(), pipeline);
            pipeline.producer_commit();
        }
        pipeline.consumer_wait();(*@\tikzmark{consume}@*)
        int shared_idx = compute_batch % stages_count;
        int batch_idx = compute_batch;
        compute(global_out + block_batch(batch_idx), shared + shared_offset[shared_idx]);
        pipeline.consumer_release();
    }
\end{lstlisting}
\begin{tikzpicture}[remember picture, overlay,
    callout/.style={rectangle callout, draw=blue!70, fill=blue!10,
                    font=\tiny, callout absolute pointer={#1},
                    rounded corners=2pt, text width=24mm, align=center}]
    \only<2->{
    \node[callout={(pic cs:twoptr)}, anchor=south] at ([shift={(0mm,3mm)}]pic cs:twoptr)
        {Two pointers:\\fetch ahead of compute};
    \node[callout={(pic cs:keepfull)}, anchor=south west] at ([shift={(3mm,2mm)}]pic cs:keepfull)
        {Keep pipeline full\\(fetch up to stages\_count ahead)};
    \node[callout={(pic cs:produce)}, anchor=west] at ([shift={(45mm,0mm)}]pic cs:produce)
        {Producer: acquire slot,\\async copy, commit};
    \node[callout={(pic cs:consume)}, anchor=west] at ([shift={(45mm,0mm)}]pic cs:consume)
        {Consumer: wait for data,\\compute, release slot};
    }
\end{tikzpicture}
\end{frame}

\begin{frame}{Unified Memory}
    \textbf{Single, shared address space between CPU and GPU.}

    \vspace{0.5em}
    \textbf{Features:}
    \begin{itemize}
        \item Both processors transparently access the same data
        \item Automatically migrates pages on-demand
        \item Enables memory overcommitment
    \end{itemize}

    \vspace{1em}
    \textbf{Considerations:}
    \begin{itemize}
        \item No hardware coherence
        \item All SM translations blocked until page-fault handled
    \end{itemize}

    \vspace{0.5em}
    \textbf{Best Practices:}
    \begin{itemize}
        \item For predictable access patterns, issue prefetch hints
        \item Pin or allocate performance-critical buffers explicitly
    \end{itemize}
\end{frame}

%===============================================================================
\section{GPU I/O}
%===============================================================================

\begin{frame}{GPGPU I/O Challenges}
    \textbf{CPU--GPU link is often a bottleneck.}

    \vspace{0.5em}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Compute (B200):}
            \begin{itemize}
                \item $\sim$2.2 PFLOP/s (FP16 Tensor)
                \item On-GPU Bandwidth: 8 TB/s
            \end{itemize}

            \vspace{0.5em}
            \textbf{Links:}
            \begin{itemize}
                \item PCIe 5.0 $\times$16: $\sim$126 GB/s
                \item NVLink 5 (Blackwell): 1.8 TB/s
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \textbf{Overheads:}
            \begin{itemize}
                \item Data transfer to/from CPU
                \item CPU involvement for disk/network I/O
                \item Synchronous I/O operations
            \end{itemize}

            \vspace{0.5em}
            \textbf{Storage:}
            \begin{itemize}
                \item NVMe SSD: 3--7 GB/s
            \end{itemize}
        \end{column}
    \end{columns}

    \vspace{1em}
    \begin{block}{Key Insight}
        Many workloads choke on data movement and delivery.
    \end{block}

    {\scriptsize Source: NVIDIA B200 Datasheet, GTC 2024}
\end{frame}

\begin{frame}{GPUDirect Storage}
    \textbf{Direct NVMe/NVMe-oF to GPU path --- bypasses CPU memory.}\\
    \textbf{GPUDirect RDMA}: Network-to-GPU without CPU involvement.\\
    {\small Without GPUDirect: CPU memory controller orchestrates two-stage copy.}

    \begin{center}
    \begin{tikzpicture}[
        node distance=15mm,
        icon/.style={inner sep=0pt},
        lbl/.style={font=\scriptsize\bfseries},
        link/.style={draw=gray!60, thick},
        bounce/.style={draw=red!70, very thick, ->},
        gds/.style={draw=green!60!black, very thick, ->},
    ]
        % === Left side: Without GPUDirect Storage ===
        \node[font=\scriptsize\bfseries] (title1) {Without GPUDirect Storage};
        \node[icon, below=5mm of title1, label={[lbl]above:Switch}] (sw1)
            {\includegraphics[height=10mm]{figures/noun-switch-7787836.png}};
        \node[icon, left of=sw1, label={[lbl]above:CPU}] (cpu1)
            {\includegraphics[height=8mm]{figures/noun-cpu-8157304.png}};
        \node[icon, below of=cpu1, label={[lbl]below:Memory}] (mem1)
            {\includegraphics[height=8mm]{figures/06-ram-dimm-icon.png}};
        \node[icon, right of=sw1, label={[lbl]above:NVMe}] (nvme1)
            {\includegraphics[height=5mm]{figures/noun-nvme-5566039.png}};
        \node[icon, below of=sw1, label={[lbl]below:GPU}] (gpu1)
            {\includegraphics[height=10mm]{figures/noun-gpu-8141519.png}};

        % Connections - left side
        \draw[link] (cpu1) -- (mem1);
        \draw[link] (cpu1) -- (sw1);
        \draw[link] (sw1) -- (nvme1);
        \draw[link] (sw1) -- (gpu1);
        % Bounce buffer path (red) - two stages via CPU (memory controller)
        % Stage 1: NVMe -> CPU -> Memory
        \draw[bounce, transform canvas={yshift=2pt}] (nvme1.west) to[out=180,in=0] (cpu1.east);
        \draw[bounce, transform canvas={xshift=-2pt}] (cpu1.south) -- (mem1.north);
        % Stage 2: Memory -> CPU -> GPU (upside-down U)
        \draw[bounce, transform canvas={xshift=2pt}] (mem1.north) to[out=90,in=180] (cpu1.east)
            to[out=0,in=90] (gpu1.north);

        % === Right side: With GPUDirect Storage ===
        \begin{scope}[xshift=55mm]
            \node[font=\scriptsize\bfseries] (title2) {With GPUDirect Storage};
            \node[icon, below=5mm of title2, label={[lbl]above:Switch}] (sw2)
                {\includegraphics[height=10mm]{figures/noun-switch-7787836.png}};
            \node[icon, left of=sw2, label={[lbl]above:CPU}] (cpu2)
                {\includegraphics[height=8mm]{figures/noun-cpu-8157304.png}};
            \node[icon, below of=cpu2, label={[lbl]below:Memory}] (mem2)
                {\includegraphics[height=8mm]{figures/06-ram-dimm-icon.png}};
            \node[icon, right of=sw2, label={[lbl]above:NVMe}] (nvme2)
                {\includegraphics[height=5mm]{figures/noun-nvme-5566039.png}};
            \node[icon, below of=sw2, label={[lbl]below:GPU}] (gpu2)
                {\includegraphics[height=10mm]{figures/noun-gpu-8141519.png}};

            % Connections - right side (inactive shown lighter)
            \draw[link, opacity=0.3] (cpu2) -- (mem2);
            \draw[link, opacity=0.3] (cpu2) -- (sw2);
            % GPUDirect path (green, direct)
            \draw[gds] (nvme2) -- (sw2);
            \draw[gds] (sw2) -- (gpu2);

            % Legend in bottom-right
            \node[anchor=north west, font=\scriptsize, align=left, right=5mm of gpu2] {
                \tikz[baseline=-0.5ex]\draw[red!70, very thick, ->] (0,0) -- (5mm,0); Bounce\\[1pt]
                \tikz[baseline=-0.5ex]\draw[green!60!black, very thick, ->] (0,0) -- (5mm,0); GPUDirect
            };
        \end{scope}
    \end{tikzpicture}
    \end{center}

    \textbf{Performance:} Up to 50 GB/s with GPUDirect Storage
\end{frame}

\begin{frame}{Multi-Instance GPU (MIG)}
    \begin{columns}
        \begin{column}{0.5\textwidth}
            \textbf{Isolation and QoS mechanism.}

            \vspace{0.5em}
            \textbf{Each instance gets its own:}
            \begin{itemize}
                \item Compute cores
                \item On-chip memory and cache
                \item Memory bandwidth
            \end{itemize}

            \vspace{0.5em}
            \textbf{Features:}
            \begin{itemize}
                \item Partitioned into different-sized instances
                \item Dynamically reconfigurable
                \item Strong isolation between workloads
            \end{itemize}

            \vspace{0.5em}
            \textbf{Limitations:}
            \begin{itemize}
                \item Limited number of instances
                \item Resources proportionally fixed
            \end{itemize}
        \end{column}
        \begin{column}{0.5\textwidth}
            \begin{center}
            \begin{tikzpicture}[
                slice/.style={draw, minimum width=8mm, minimum height=6mm, font=\footnotesize\ttfamily},
                gpu/.style={slice, fill=orange!50},
                mem/.style={slice, fill=gray!25},
                grp/.style={draw, thick, rounded corners=2pt, inner sep=2pt},
                joblbl/.style={font=\tiny\bfseries, text depth=0.25ex},
            ]
                % GPU slices row
                \matrix[matrix of nodes, nodes={gpu}, column sep=1pt, row sep=1pt,
                        ampersand replacement=\&] (gpus) {
                    GPU \& GPU \& GPU \& GPU \& GPU \& GPU \& GPU \\
                };
                % Memory slices row
                \matrix[matrix of nodes, nodes={mem}, column sep=1pt, row sep=1pt,
                        ampersand replacement=\&, below=1pt of gpus] (mems) {
                    Mem \& Mem \& Mem \& Mem \& Mem \& Mem \& Mem \\
                };

                % Workload grouping boxes on background layer
                \begin{scope}[on background layer]
                    \node[grp, fill=blue!15, fit=(gpus-1-1) (gpus-1-3) (mems-1-1) (mems-1-3),
                          label={[joblbl]above:Job A (3g)}] {};
                    \node[grp, fill=green!15, fit=(gpus-1-4) (gpus-1-5) (mems-1-4) (mems-1-5),
                          label={[joblbl]above:Job B (2g)}] {};
                    \node[grp, fill=purple!15, fit=(gpus-1-6) (mems-1-6),
                          label={[joblbl]above:Job C}] {};
                    \node[grp, fill=gray!10, dashed, fit=(gpus-1-7) (mems-1-7),
                          label={[font=\tiny, text depth=0.25ex]above:Free}] {};
                \end{scope}

                % Label
                \node[below=4mm of mems, font=\scriptsize] {Example: 3g + 2g + 1g + 1g partition};
            \end{tikzpicture}
            \end{center}
        \end{column}
    \end{columns}
\end{frame}

%===============================================================================
\section{Summary}
%===============================================================================

\begin{frame}{Summary}
    \begin{itemize}
        \item \textbf{GPGPU}: Massive parallelism for data-parallel workloads
        \item \textbf{Roofline Model}: Analyze compute vs memory-bound kernels
        \item \textbf{Warp Scheduling}: Latency hiding through occupancy
        \item \textbf{Divergence}: Avoid branches within warps
        \item \textbf{Tensor Cores}: Accelerated matrix operations (FP16, BF16, TF32)
        \item \textbf{HBM}: High-bandwidth memory (3+ TB/s)
        \item \textbf{Async Memory}: Overlap compute and data movement
        \item \textbf{GPUDirect}: Bypass CPU for storage and network I/O
        \item \textbf{MIG}: Partition GPU for isolation and QoS
    \end{itemize}

    \vspace{1em}
    \begin{block}{Conclusion}
        GPUs provide massive parallelization with unique challenges in compute (divergence) and I/O (limited links). They are increasingly becoming ``first-class citizens'' in modern computing.
    \end{block}
\end{frame}

\end{document}
